classdef DatabaseAnalysis < handle & DataSource

    properties
        % time which the analysis started running
        timeRun

        % used by callbacks to store information by entry
        currentEntry
        currentEntryIndex

        % the result table will be an instance of a loadOnDemandMapped Table
        resultTable

        successByEntry
        exceptionByEntry
        diaryFileByEntry = {};
        logByEntry = {};

        % each element contains a struct with fields
        %   .pathNoExt
        %   .fileNameNoExt
        %   .extensions
        %   .name
        %   .caption
        %   .width
        %   .height
        figureInfoByEntry = {};

        figureRootPaths  = {'~/npl/analysis/', '/net/home/djoshea/npl/analysis'};
        figureExtensions = {'png', 'eps', 'svg'};
    end

    properties(Transient)
        % reference to the current database when running
        database 
    end

    properties(Dependent)
        fieldsAnalysis 
    end

    methods(Abstract)
        % return a single word descriptor for this analysis, ignoring parameter
        % settings in param. The results will be stored as a DataTable with this
        % as the entryName
        name = getName(da);

        % return the param to be used when caching
        param = getCacheParam(da);
        
        % return the entryName corresponding to the table in the database which this
        % analysis runs on. The DataTable with this entry name will run this analysis
        % once on each entry and map the results via a 1-1 relationship 
        entryName = getMapsEntryName(da);
    
        % return a list of fields generated by the analysis. These need to be declared
        % ahead of time to simplify many of the caching related features.
        [fields fieldDescriptorMap] = getFieldsAnalysis(da);

        % run this analysis on one entry, entryTable will be a DataTable instance
        % filtered down to one entry
        resultStruct = runOnEntry(da, entryTable, fields)

    end

    methods % not necessary to override if the defaults are okay
        function tf = isequal(da, other)
            tf = true;
            if ~strcmp(class(da), class(other))
                tf = false;
                return;
            end
            if ~isequal(da.getCacheParam(), other.getCacheParam())
                tf = false;
                return;
            end
        end

        function tf = getRerunCachedUnsuccessful(da)
            tf = false;
        end

        % returns a list of entryNames that this analysis references
        % this is used when retrieving analysis results from the cache. If a related
        % table has changed, the analysis will be rerun on all entries.
        function list = getReferencesRelatedEntryNames(da)
            list = {};
        end

        % An analysis runs on each entry of a specific data table, and may reference
        % related tables through relationships in the database. When any of these
        % data tables is modified, this could invalidate the results of an analysis
        % for some or all of the entries. However, for caching to be at all useful
        % we simply issue a warning when using cached analysis results that references
        % a datatable that has changed. We assume that all changes have been additive
        % and do not affect the analysis that has already been run. Thus we only 
        % run the analysis on entries that are missing related rows in this analysis.
        % 
        % However, if this will yield incorrect results, you can specify that
        % when certain tables are changed at all, the entire cached analysis 
        % becomes invalid. Return a list of the .entryName of these tables here.
        function list = getEntryNamesChangesInvalidateCache(da)
            list = {};
        end
        
        % return a cell array of DataSource instances that must be loaded before 
        % running this analysis
        function sources = getRequiredSources(da)
            sources = {};
        end
        
        % return a cell array of DatabaseView instances that must be applied before 
        % running this analysis
        function views = getRequiredViews(da)
            views = {};
        end
        
        % filter the data table as necessary before this analysis is run
        % table.entryName will match the result of getMapsEntryName() above
        % 
        % If the filtering pattern is a common one, consider turning it into a 
        % DatabaseView class and returning an instance from getDatabaseViewsToApply()
        function table = preFilterTable(da, table)
            % default does nothing
        end

        % return a list of additional meta fields that resultTable will contain
        % in addition to the analysis field and keyFields
        function [fields fieldDescriptorMap] = getFieldsAdditional(da, table)
            fieldDescriptorMap = ValueMap('KeyType', 'char', 'ValueType', 'any');
            fieldDescriptorMap('success') = BooleanField();
            fieldDescriptorMap('output') = OutputField();
            fieldDescriptorMap('runTimestamp') = DateTimeField();
            fieldDescriptorMap('exception') = UnspecifiedField();
            fieldDescriptorMap('figureInfo') = UnspecifiedField();
            fields = fieldDescriptorMap.keys;
        end
    end

    methods
        function resultTable = run(da, db, varargin)
            p = inputParser();
            p.addRequired('db', @(db) isa(db, 'Database'));
            % optionally select subset of fields for analysis
            p.addParamValue('fields', da.fieldsAnalysis, @iscellstr); 
            p.addParamValue('loadCache', true, @islogical); 
            p.addParamValue('saveCache', true, @islogical); 
            p.addParamValue('rerunFailed', false, @islogical); 
            p.addParamValue('loadCacheOnly', false, @islogical);
            p.parse(db, varargin{:});

            fieldsAnalysis = p.Results.fields;
            loadCache = p.Results.loadCache;
            saveCache = p.Results.saveCache;
            rerunFailed = p.Results.rerunFailed;
            loadCacheOnly = p.Results.loadCacheOnly;

            % get analysis name
            name = da.getName();
            assert(isvarname(name), 'getName() must return a valid variable name');
            debug('Preparing for analysis : %s\n', name);

            da.database = db;
            % mark run timestamp
            da.timeRun = now;
            fieldsAdditional = da.getFieldsAdditional();
            
            % load all data sources
            db.loadSource(da.getRequiredSources());
            
            % load all data views 
            db.applyView(da.getRequiredViews());

            % get the appropriate table to map
            entryName = da.getMapsEntryName();
            table = db.getTable(entryName);

            % prefilter the table further if requested (database views also do this)
            table = da.preFilterTable(table);

            % build the resultTable as a LoadOnDemandTable
            % this will be a skeleton containing all of the fields for the analysis
            % none of which will be loaded initially
            resultTable = DatabaseAnalysisResultsTable(da);

            % now we ask resultTable load whatever it can from the cache
            if loadCache
                % load the table itself from cache and copy over additional field
                % values from the cache hit if present
                if resultTable.hasCache()
                    cachedTable = resultTable.loadFromCache();

                    % using temporary copy of resultTable, create a one to one
                    % relationship to handle the correspondence easily
                    dbTemp = Database();
                    resultTableCopy = resultTable;
                    resultTableCopy = resultTableCopy.setEntryName('emptyResult');
                    cachedTable = cachedTable.setEntryName('cachedResult');
                    cachedTable = dbTemp.addTable(cachedTable);
                    resultTableCopy = dbTemp.addTable(resultTableCopy);
                    dbTemp.addRelationshipOneToOne(resultTableCopy.entryName, cachedTable.entryName);
                    
                    % for each matching entry in cachedTable, copy over the values of
                    % all additional fields, use the 1:1 relationship to find matches
                    matchCell = resultTableCopy.matchRelated('cachedResult', 'combine', false);
                    for iEntry = 1:resultTable.nEntries
                        if matchCell{iEntry}.nEntries > 0
                            debug('Copying match for row %d in cached table\n', iEntry);
                            match = matchCell{iEntry};
                            for iField = 1:length(fieldsAdditional)
                                field = fieldsAdditional{iField};
                                resultTable = resultTable.setFieldValue(iEntry, field, match.(field));
                            end
                        end
                    end
                end

                % now we search for field values in fieldsAnalysis in the cache
                debug('Loading analysis field values from cache\n');
                % have the table load values from cache, but not call the loadValue
                % callback to request the field value as we'll run the actual
                % analysis below
                resultTable = resultTable.loadFields('loadCacheOnly', true);

                % get information about which field values were loaded from cache
                loadedByEntry = resultTable.loadedByEntry;
                cacheTimestamps = resultTable.cacheTimestampsByEntry;

                % check whether we should warn about modifications that could
                % theoretically affect the cached analysis but weren't explicitly
                % returned by getEntryNamesChangesInvalidateCache()
                references = makecol(da.getReferencesRelatedEntryNames());
                relevantTableList = [entryName; references];
                lastRelevantUpdate = db.getLastUpdated(relevantTableList);

                % calculate the oldest cache value loaded 
                fieldCacheOlderThanTableCount = 0;
                for iField = 1:length(fieldsAnalysis)
                    field = fieldsAnalysis{iField};
                    for iEntry = 1:resultTable.nEntries
                        if loadedByEntry(iEntry).(field)
                            timestamp = cacheTimestamps(iEntry).(field);
                            if timestamp < lastRelevantUpdate
                                fieldCacheOlderThanTableCount = fieldCacheOlderThanTableCount + 1;  
                            end
                        end
                    end
                end
                if fieldCacheOlderThanTableCount > 0 
                    debug('Warning: This DatabaseAnalysis has field values which were loaded from cache, but references tables modified after %d cached values were written. Use .deleteCache() to force a full re-run.\n', ...
                        fieldCacheOlderThanTableCount);
                end
            end

            % analyze entries that haven't been loaded from cache?
            if ~loadCacheOnly
                % figure out which entries have not yet been loaded
                loadedByEntry = resultTable.loadedByEntry;
                maskToAnalyze = true(resultTable.nEntries, 1);
                for iEntry = 1:resultTable.nEntries
                    allLoaded = true; 
                    for iField = 1:length(fieldsAnalysis)
                        field = fieldsAnalysis{iField};
                        if ~loadedByEntry(iEntry).(field)
                            allLoaded = false;
                            break;
                        end
                    end
                    maskToAnalyze(iEntry) = ~allLoaded;
                end
                        
                % do we re-run failed runs from last time
                if da.getRerunCachedUnsuccessful() || rerunFailed
                    maskFailed = makecol(~resultTable.success);
                    debug('Rerunning on %d entries which failed last time\n', nnz(maskFailed));
                    % also reanalyze any rows which were listed as unsuccessful
                    maskToAnalyze = maskToAnalyze | maskFailed;
                end
                    
                % now need to run the analysis on entries in maskToAnalyze
                tableAnalyze = table.select(maskToAnalyze);
                
                if tableAnalyze.nEntries > 0
                    % create slots for by entry information that must be captured from
                    % within the mapped function (runOnEntryWrapper)
                    da.figureInfoByEntry = cell(tableAnalyze.nEntries, 1);
                    da.diaryFileByEntry = cell(tableAnalyze.nEntries, 1);

                    debug('Running analysis %s on %d of %d %s entries\n', name, ...
                        tableAnalyze.nEntries, table.nEntries, entryName);
                    
                    % run the analysis
                    entryWrapperFn = @(varargin) da.runOnEntryWrapper(varargin{:}, ...
                        'fields', fieldsAnalysis);
                    [newResultTable statusByEntry] = tableAnalyze.map(entryWrapperFn, ...
                        'entryName', name, 'entryNamePlural', name, ...
                        'addToDatabase', false);

                    % read from the diary files the output for each entry
                    logByEntry = da.loadDiaryFiles();
                    figureInfoByEntry = da.figureInfoByEntry;

                    % add meta fields to the table
                    % 'success' : boolean indicating whether the run was successful
                    % 'output' : contains the raw output to the command window
                    % 'exception' : contains any exceptions thrown
                    % 'figureInfo' : contains a struct array with info about the figures saved
                    newResultTable = newResultTable.addField('success', [statusByEntry.success] > 0, ...
                        'position', 1, 'fieldDescriptor', BooleanField());
                    newResultTable = newResultTable.addField('output', logByEntry, ...
                        'position', 2, 'fieldDescriptor', OutputField());
                    newResultTable = newResultTable.addField('runTimestamp', da.timeRun, ...
                        'position', 3, 'fieldDescriptor', DateTimeField());
                    newResultTable = newResultTable.addField('exception', {statusByEntry.exception}, ...
                        'position', 4, 'fieldDescriptor', UnspecifiedField());
                    newResultTable = newResultTable.addField('figureInfo', figureInfoByEntry, ...
                        'position', 5, 'fieldDescriptor', UnspecifiedField());
                    
                    % store every value in newResultTable into the appropriate slot
                    % in resultTable
                    newInOldIdx = find(maskToAnalyze);
                    for iNew = 1:newResultTable.nEntries
                        iOld = newInOldIdx(iNew);
                        % first set analysis fields
                        fieldsSet = union(fieldsAnalysis, fieldsAdditional);
                        for iField = 1:length(fieldsSet)
                            field = fieldsSet{iField}; 
                            resultTable = resultTable.setFieldValue(iOld, ...
                                field, newResultTable{iNew}.(field), ...
                                'saveCache', saveCache);
                        end
                    end
                end
            end

            % add the result table to the database 
            %resultTable = da.integrateIntoDatabase(resultTable, db);

            % now fill in all of the info fields of this class with the full table data
            da.successByEntry = resultTable.getValues('success') > 0;                
            da.exceptionByEntry = resultTable.getValues('exception');
            da.figureInfoByEntry = resultTable.getValues('figureInfo');
            da.logByEntry = resultTable.getValues('output');
            da.resultTable = resultTable;

            % mark loaded in database
            da.database.markSourceLoaded(da);

            if saveCache
                da.resultTable.cache('cacheValues', false);
            end
        end

        function resultStruct = runOnEntryWrapper(da, entry, entryIndex, varargin)
            p = inputParser;
            p.addParamValue('fields', da.fieldsAnalysis, @iscellstr);
            p.parse(varargin{:});
            fields = p.Results.fields;
            da.currentEntryIndex = entryIndex; 
            da.currentEntry = entry;
            
            % open a temporary file to use as a diary to capture all output
            diary off;
            diaryFile = tempname(); 
            da.diaryFileByEntry{da.currentEntryIndex} = diaryFile; 
            diary(diaryFile);

            resultStruct = da.runOnEntry(entry, fields);

            % if there's an exception, we won't get here, so load all of the 
            % diary files later in run()
            diary('off');
        end

        function resultTable = integrateIntoDatabase(da, resultTable, db)
            resultTable = resultTable.setDatabase(db);
            resultTable.updateInDatabase();
            mapsName = da.getMapsEntryName();
            db.addRelationshipOneToOne(mapsName, resultTable.entryName);
        end

        function logByEntry = loadDiaryFiles(da)
            diary('off');
            logByEntry = cell(length(da.diaryFileByEntry), 1);
            for i = 1:length(da.diaryFileByEntry)
                file = da.diaryFileByEntry{i};
                logByEntry{i} = fileread(file);
            end
        end

        function saveFigure(da, figh, figName, figCaption)
            % use this to save figures while running the analysis
            if nargin < 4
                figCaption = '';
            end

            entryTable = da.currentEntry;

            assert(entryTable.nEntries == 1);
            fileNameNoExt = da.getFigureNameNoExt(entryTable, figName);
            mkdirRecursive(fileparts(fileNameNoExt));

            exts = da.figureExtensions;
            debug('Saving figure %s as %s\n', figName, strjoin(exts, ', '));
            for i = 1:length(exts)
                ext = exts{i};
                fileName = [fileNameNoExt '.' ext];
                if strcmp(ext, 'svg')
                    try
                        plot2svg(fileName, figh);
                    catch exc
                        warning('Error saving to svg');
                        fprintf(exc.getReport());
                    end
                else
                    try
                        exportfig(figh, fileName, 'format', ext, 'resolution', 300);
                    catch exc
                        warning('Error saving to %s', ext);
                        fprintf(exc.getReport());
                    end
                end
            end

            % log figure infomration
            figInfo.pathNoExt = GetFullPath(fileNameNoExt);
            [~, figInfo.fileNameNoExt] = fileparts(figInfo.pathNoExt);
            figInfo.name = figName;
            figInfo.caption = figCaption;
            figInfo.extensions = exts;
            [figInfo.width figInfo.height] = getFigSize(figh);

            % add to figure info cell
            da.figureInfoByEntry{da.currentEntryIndex}(end+1) = orderfields(figInfo);
        end

        function fileName = getFigureNameNoExt(da, entryTable, figName)
            assert(entryTable.nEntries == 1);
            analysisName = da.getName();
            descriptors = entryTable.getKeyFieldValueDescriptors();
            timestamp = datestr(da.timeRun, 'yyyy-mm-dd/HH.MM.SS');

            figureRootPath = getFirstExisting(da.figureRootPaths);
            path = fullfile(figureRootPath, timestamp, analysisName);
            
            figNameFn = @(descriptor) fullfile(path, sprintf('%s.%s', figName, descriptor));
            fileName = figNameFn(descriptors{1}); 
        end

        function viewAsHtml(da)
            fileName = [tempname() '.html'];
            html = da.saveAsHtml(fileName);
            html.openInBrowser();
        end
        
        function html = saveAsHtml(da, fileName)
            html = HTMLDatabaseAnalysisWriter(fileName);
            html.generate(da);
        end

        function disp(da)
            fprintf('DatabaseAnalysis : %s on %s\n\n', da.getName(), da.getMapsEntryName());
        end
    end

    methods % Cacheable instantiations
        % return the cacheName to be used when instance 
        function name = getCacheName(obj)
            name = obj.getName();
        end


        function timestamp = getCacheValidAfterTimestamp(obj)
            % my data is valid until the last modification timestamp of the 
            if isempty(obj.database)
                % shouldn't happen when running normally, but could if cache functions
                % are called directly
                debug('Warning: Unable to determine whether analysis cache is valid because no .database found');
                % invalidate the cache
                timestamp = Inf;
            else
                % loop through these tables and find the latest modification time
                list = obj.getEntryNamesChangesInvalidateCache();
                
                % ask the database when the latest modification to these tables was
                timestamp = obj.database.getLastUpdated(list);
            end
        end
    end

    methods % Dependent properties
        function fields = get.fieldsAnalysis(da)
            fields = da.getFieldsAnalysis();
        end
    end

    methods % DataSource instantiations 
        % return a string describing this datasource
        function str = describe(da)
            str = da.getName();
        end

        % actually load this into the database, assume all dependencies have been loaded
        function loadInDatabase(da, database)
            da.run(database);
        end
    end
end
